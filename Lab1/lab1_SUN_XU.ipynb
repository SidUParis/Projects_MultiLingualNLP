{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How is the TTR (type-token ratio) defined? Why is this a ‘good’ measure of the morphological complexity? Which other metric could you use to quantify morphological complexity?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "a. Type-Token-Ratio is defined by the total number of unique words(type) divided by the total number of words(tokens) in a given segment of language. It is a simple measurement of the lexical diversity. \n",
    "    \n",
    "TTR = (Number of unique words) / (Total number of words)\n",
    "\n",
    "        Ref: https://carla.umn.edu/learnerlanguage/spn/comp/activity4.\n",
    "        Ref: https://www.researchgate.net/profile/Kimmo-Kettunen-2/publication/263169986_Can_Type-Token_Ratio_be_Used_to_Show_Morphological_Complexity_of_Languages/links/0046353a1779defb1d000000/Can-Type-Token-Ratio-be-Used-to-Show-Morphological-Complexity-of-Languages.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Why is this a 'good' measure of the morphological complexity? \n",
    "\n",
    "**Answer**: \n",
    "    1. Morphological Complexity means the language has a lot of inflection/ more structural units, rules or representations\n",
    "    2. TTR methods correlates well with other established mesure of Morphological Complexity, such as Juola's mesure schema.\n",
    "    3. TTR reflects the varitety of word forms in languages \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Which other metric could you use to quantify morphological complexity? \n",
    "\n",
    "**Answer**: \n",
    "    In the Measurement of the morphological complexity, a popular algo proposed by Juola is ： Distort words using unique random numbers for each word type, Compress both the distorted and original data, and compare the compressed file size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 What Conclusion Can you draw from Fig.1?\n",
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Why will we not consider the corpus used in [1]? Why are the FAIR principles1 a solution to this problem?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T00:04:02.293779Z",
     "start_time": "2024-09-26T00:04:02.177321Z"
    }
   },
   "source": [
    "import tensorflow_datasets as tfds \n",
    "from itertools import islice\n",
    "\n",
    "ds = tfds.load(\"wiki40b/fr\",split=\"test\",data_dir = \"gs://tfds-data/datasets\")\n",
    "sample = [ex['text'].numpy().decode('utf-8') for ex in islice(ds.shuffle(buffer_size=10_000),100)]"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "UnimplementedError: Failed to construct dataset wiki40b: File system scheme 'gs' not implemented (file: 'gs://tfds-data/datasets\\wiki40b\\fr')",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnimplementedError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:432\u001B[0m, in \u001B[0;36mtry_reraise\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 432\u001B[0m   \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:210\u001B[0m, in \u001B[0;36mbuilder\u001B[1;34m(name, try_gcs, **builder_kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m py_utils\u001B[38;5;241m.\u001B[39mtry_reraise(prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFailed to construct dataset \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 210\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs)  \u001B[38;5;66;03m# pytype: disable=not-instantiable\u001B[39;00m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;66;03m# If neither the code nor the files are found, raise DatasetNotFoundError\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:288\u001B[0m, in \u001B[0;36mbuilder_init.<locals>.decorator\u001B[1;34m(function, dsbuilder, args, kwargs)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 288\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1220\u001B[0m, in \u001B[0;36mFileReaderBuilder.__init__\u001B[1;34m(self, file_format, **kwargs)\u001B[0m\n\u001B[0;32m   1210\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Initializes an instance of FileReaderBuilder.\u001B[39;00m\n\u001B[0;32m   1211\u001B[0m \n\u001B[0;32m   1212\u001B[0m \u001B[38;5;124;03mCallers must pass arguments as keyword arguments.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;124;03m  **kwargs: Arguments passed to `DatasetBuilder`.\u001B[39;00m\n\u001B[0;32m   1219\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1220\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mset_file_format(file_format)\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:288\u001B[0m, in \u001B[0;36mbuilder_init.<locals>.decorator\u001B[1;34m(function, dsbuilder, args, kwargs)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 288\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:255\u001B[0m, in \u001B[0;36mDatasetBuilder.__init__\u001B[1;34m(self, data_dir, config, version)\u001B[0m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;66;03m# Compute the base directory (for download) and dataset/version directory.\u001B[39;00m\n\u001B[1;32m--> 255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_dir_root, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_data_dir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_path\u001B[38;5;241m.\u001B[39mexists():\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:928\u001B[0m, in \u001B[0;36mDatasetBuilder._build_data_dir\u001B[1;34m(self, given_data_dir)\u001B[0m\n\u001B[0;32m    927\u001B[0m full_builder_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_dir_root, builder_dir)\n\u001B[1;32m--> 928\u001B[0m data_dir_versions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_all_versions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_builder_dir\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    929\u001B[0m \u001B[38;5;66;03m# Check for existance of the requested version\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\version.py:193\u001B[0m, in \u001B[0;36mlist_all_versions\u001B[1;34m(root_dir)\u001B[0m\n\u001B[0;32m    192\u001B[0m root_dir \u001B[38;5;241m=\u001B[39m epath\u001B[38;5;241m.\u001B[39mPath(root_dir)\n\u001B[1;32m--> 193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mroot_dir\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexists\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    194\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m []\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\etils\\epath\\gpath.py:144\u001B[0m, in \u001B[0;36m_GPath.exists\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns True if self exists.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexists\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_path_str\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\etils\\epath\\backend.py:225\u001B[0m, in \u001B[0;36m_TfBackend.exists\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexists\u001B[39m(\u001B[38;5;28mself\u001B[39m, path: PathLike) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m--> 225\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:288\u001B[0m, in \u001B[0;36mfile_exists_v2\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 288\u001B[0m   \u001B[43m_pywrap_file_io\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFileExists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_to_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mNotFoundError:\n",
      "\u001B[1;31mUnimplementedError\u001B[0m: File system scheme 'gs' not implemented (file: 'gs://tfds-data/datasets\\wiki40b\\fr')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtfds\u001B[39;00m \n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m islice\n\u001B[1;32m----> 4\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwiki40b/fr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgs://tfds-data/datasets\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m sample \u001B[38;5;241m=\u001B[39m [ex[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m ex \u001B[38;5;129;01min\u001B[39;00m islice(ds\u001B[38;5;241m.\u001B[39mshuffle(buffer_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10_000\u001B[39m),\u001B[38;5;241m100\u001B[39m)]\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[1;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    167\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 169\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    171\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:614\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[0;32m    611\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m builder_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    612\u001B[0m   builder_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m--> 614\u001B[0m dbuilder \u001B[38;5;241m=\u001B[39m builder(name, data_dir\u001B[38;5;241m=\u001B[39mdata_dir, try_gcs\u001B[38;5;241m=\u001B[39mtry_gcs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs)\n\u001B[0;32m    615\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[0;32m    616\u001B[0m   download_and_prepare_kwargs \u001B[38;5;241m=\u001B[39m download_and_prepare_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\contextlib.py:79\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[1;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[1;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    167\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 169\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    171\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:210\u001B[0m, in \u001B[0;36mbuilder\u001B[1;34m(name, try_gcs, **builder_kwargs)\u001B[0m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m:\n\u001B[0;32m    209\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m py_utils\u001B[38;5;241m.\u001B[39mtry_reraise(prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFailed to construct dataset \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 210\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs)  \u001B[38;5;66;03m# pytype: disable=not-instantiable\u001B[39;00m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;66;03m# If neither the code nor the files are found, raise DatasetNotFoundError\u001B[39;00m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m not_found_error\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\contextlib.py:137\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    135\u001B[0m     value \u001B[38;5;241m=\u001B[39m typ()\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    139\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m value\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:434\u001B[0m, in \u001B[0;36mtry_reraise\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    432\u001B[0m   \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m--> 434\u001B[0m   reraise(e, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\labsession\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:401\u001B[0m, in \u001B[0;36mreraise\u001B[1;34m(e, prefix, suffix)\u001B[0m\n\u001B[0;32m    399\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    400\u001B[0m     exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(e)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 401\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m exception \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    402\u001B[0m \u001B[38;5;66;03m# Otherwise, modify the exception in-place\u001B[39;00m\n\u001B[0;32m    403\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(e\u001B[38;5;241m.\u001B[39margs) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: UnimplementedError: Failed to construct dataset wiki40b: File system scheme 'gs' not implemented (file: 'gs://tfds-data/datasets\\wiki40b\\fr')"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T00:01:26.764582Z",
     "start_time": "2024-09-26T00:01:26.736583Z"
    }
   },
   "source": [
    "tfds.list_builders()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'accentdb',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ag_news_subset',\n",
       " 'ai2_arc',\n",
       " 'ai2_arc_with_ir',\n",
       " 'amazon_us_reviews',\n",
       " 'anli',\n",
       " 'answer_equivalence',\n",
       " 'arc',\n",
       " 'asqa',\n",
       " 'asset',\n",
       " 'assin2',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bccd',\n",
       " 'beans',\n",
       " 'bee_dataset',\n",
       " 'beir',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'ble_wind_field',\n",
       " 'blimp',\n",
       " 'booksum',\n",
       " 'bool_q',\n",
       " 'bucc',\n",
       " 'c4',\n",
       " 'c4_wsrs',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cardiotox',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'cherry_blossoms',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar100_n',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'cifar10_n',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'clic',\n",
       " 'clinc_oos',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco_captions',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'conll2002',\n",
       " 'conll2003',\n",
       " 'controlled_noisy_web_labels',\n",
       " 'coqa',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'covid19',\n",
       " 'covid19sum',\n",
       " 'crema_d',\n",
       " 'criteo',\n",
       " 'cs_restaurants',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'd4rl_adroit_door',\n",
       " 'd4rl_adroit_hammer',\n",
       " 'd4rl_adroit_pen',\n",
       " 'd4rl_adroit_relocate',\n",
       " 'd4rl_antmaze',\n",
       " 'd4rl_mujoco_ant',\n",
       " 'd4rl_mujoco_halfcheetah',\n",
       " 'd4rl_mujoco_hopper',\n",
       " 'd4rl_mujoco_walker2d',\n",
       " 'dart',\n",
       " 'davis',\n",
       " 'deep1b',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'diamonds',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'doc_nli',\n",
       " 'dolphin_number_word',\n",
       " 'domainnet',\n",
       " 'downsampled_imagenet',\n",
       " 'drop',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'e2e_cleaned',\n",
       " 'efron_morris75',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'fuss',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'gem',\n",
       " 'genomics_ood',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glove100_angular',\n",
       " 'glue',\n",
       " 'goemotions',\n",
       " 'gov_report',\n",
       " 'gpt3',\n",
       " 'gref',\n",
       " 'groove',\n",
       " 'grounded_scan',\n",
       " 'gsm8k',\n",
       " 'gtzan',\n",
       " 'gtzan_music_speech',\n",
       " 'hellaswag',\n",
       " 'higgs',\n",
       " 'hillstrom',\n",
       " 'horses_or_humans',\n",
       " 'howell',\n",
       " 'i_naturalist2017',\n",
       " 'i_naturalist2018',\n",
       " 'i_naturalist2021',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_fewshot',\n",
       " 'imagenet2012_multilabel',\n",
       " 'imagenet2012_real',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_a',\n",
       " 'imagenet_lt',\n",
       " 'imagenet_r',\n",
       " 'imagenet_resized',\n",
       " 'imagenet_sketch',\n",
       " 'imagenet_v2',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'irc_disentanglement',\n",
       " 'iris',\n",
       " 'istella',\n",
       " 'kddcup99',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'laion400m',\n",
       " 'lambada',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'locomotion',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'lvis',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'math_qa',\n",
       " 'mctaco',\n",
       " 'media_sum',\n",
       " 'mlqa',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_lens',\n",
       " 'movie_rationales',\n",
       " 'movielens',\n",
       " 'moving_mnist',\n",
       " 'mrqa',\n",
       " 'mslr_web',\n",
       " 'mt_opt',\n",
       " 'mtnt',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_instructions',\n",
       " 'natural_questions',\n",
       " 'natural_questions_open',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'nyu_depth_v2',\n",
       " 'ogbg_molpcba',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'openbookqa',\n",
       " 'opinion_abstracts',\n",
       " 'opinosis',\n",
       " 'opus',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'pass',\n",
       " 'patch_camelyon',\n",
       " 'paws_wiki',\n",
       " 'paws_x_wiki',\n",
       " 'penguins',\n",
       " 'pet_finder',\n",
       " 'pg19',\n",
       " 'piqa',\n",
       " 'places365_small',\n",
       " 'placesfull',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'protein_net',\n",
       " 'q_re_cc',\n",
       " 'qa4mre',\n",
       " 'qasc',\n",
       " 'quac',\n",
       " 'quality',\n",
       " 'quickdraw_bitmap',\n",
       " 'race',\n",
       " 'radon',\n",
       " 'reddit',\n",
       " 'reddit_disentanglement',\n",
       " 'reddit_tifu',\n",
       " 'ref_coco',\n",
       " 'resisc45',\n",
       " 'rlu_atari',\n",
       " 'rlu_atari_checkpoints',\n",
       " 'rlu_atari_checkpoints_ordered',\n",
       " 'rlu_control_suite',\n",
       " 'rlu_dmlab_explore_object_rewards_few',\n",
       " 'rlu_dmlab_explore_object_rewards_many',\n",
       " 'rlu_dmlab_rooms_select_nonmatching_object',\n",
       " 'rlu_dmlab_rooms_watermaze',\n",
       " 'rlu_dmlab_seekavoid_arena01',\n",
       " 'rlu_locomotion',\n",
       " 'rlu_rwrl',\n",
       " 'robomimic_ph',\n",
       " 'robonet',\n",
       " 'robosuite_panda_pick_place_can',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 's3o4d',\n",
       " 'salient_span_wikipedia',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'schema_guided_dialogue',\n",
       " 'sci_tail',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'scrolls',\n",
       " 'sentiment140',\n",
       " 'shapes3d',\n",
       " 'sift1m',\n",
       " 'simpte',\n",
       " 'siscore',\n",
       " 'smallnorb',\n",
       " 'smartwatch_gestures',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'spoken_digit',\n",
       " 'squad',\n",
       " 'squad_question_generation',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'star_cfq',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'story_cloze',\n",
       " 'summscreen',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'symmetric_solids',\n",
       " 'tao',\n",
       " 'tatoeba',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tydi_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'unified_qa',\n",
       " 'universal_dependencies',\n",
       " 'unnatural_instructions',\n",
       " 'user_libri_audio',\n",
       " 'user_libri_text',\n",
       " 'vctk',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'voxforge',\n",
       " 'waymo_open_dataset',\n",
       " 'web_graph',\n",
       " 'web_nlg',\n",
       " 'web_questions',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wiki_auto',\n",
       " 'wiki_bio',\n",
       " 'wiki_dialog',\n",
       " 'wiki_table_questions',\n",
       " 'wiki_table_text',\n",
       " 'wikiann',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes',\n",
       " 'wine_quality',\n",
       " 'winogrande',\n",
       " 'wit',\n",
       " 'wit_kaggle',\n",
       " 'wmt13_translate',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'wordnet',\n",
       " 'wsc273',\n",
       " 'xnli',\n",
       " 'xquad',\n",
       " 'xsum',\n",
       " 'xtreme_pawsx',\n",
       " 'xtreme_pos',\n",
       " 'xtreme_s',\n",
       " 'xtreme_xnli',\n",
       " 'yahoo_ltrc',\n",
       " 'yelp_polarity_reviews',\n",
       " 'yes_no',\n",
       " 'youtube_vis']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
