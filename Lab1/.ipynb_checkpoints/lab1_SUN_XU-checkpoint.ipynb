{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m运行具有“ud2-embed (Python 3.9.19)”的单元格需要ipykernel包。\n",
      "\u001B[1;31m运行以下命令，将 \"ipykernel\" 安装到 Python 环境中。\n",
      "\u001B[1;31m命令: \"conda install -n ud2-embed ipykernel --update-deps --force-reinstall\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 How is the TTR defined? \n",
    "\n",
    "**Answer**: Type-Token-Ratio is the total number of unique words(type) divided by the total number of words(tokens) in a given segment of language\n",
    "\n",
    "Ref: https://carla.umn.edu/learnerlanguage/spn/comp/activity4.html\n",
    "\n",
    "Type-Token-Ratio is the vocabulary size divided by the text length, it is a simple measurement of the lexical diversity. \n",
    "\n",
    "Ref: https://www.researchgate.net/profile/Kimmo-Kettunen-2/publication/263169986_Can_Type-Token_Ratio_be_Used_to_Show_Morphological_Complexity_of_Languages/links/0046353a1779defb1d000000/Can-Type-Token-Ratio-be-Used-to-Show-Morphological-Complexity-of-Languages.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Why is this a 'good' measure of the morphological complexity? \n",
    "\n",
    "**Answer**: \n",
    "    1. Morphological Complexity means the language has a lot of inflection/ more structural units, rules or representations\n",
    "    2. TTR methods correlates well with other established mesure of Morphological Complexity, such as Juola's mesure schema.\n",
    "    3. TTR reflects the varitety of word forms in languages \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Which other metric could you use to quantify morphological complexity? \n",
    "\n",
    "**Answer**: \n",
    "    In the Measurement of the morphological complexity, a popular algo proposed by Juola is ： Distort words using unique random numbers for each word type, Compress both the distorted and original data, and compare the compressed file size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 What Conclusion Can you draw from Fig.1?\n",
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Why will we not consider the corpus used in [1]? Why are the FAIR principles1 a solution to this problem?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T23:46:10.460203Z",
     "start_time": "2024-09-25T23:46:09.705516Z"
    }
   },
   "source": [
    "import tensorflow_datasets as tfds \n",
    "from itertools import islice\n",
    "\n",
    "ds = tfds.load(\"wiki40b/fr\",split=\"test\",data_dir = \"https://tfhub.dev/google/collections/wiki40b-lm/1\")\n",
    "sample = [ex['text'].numpy().decode('utf-8') for ex in islice(ds.shuffle(buffer_size=10_000),100)]"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resource'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtfds\u001B[39;00m \n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m islice\n\u001B[0;32m      4\u001B[0m ds \u001B[38;5;241m=\u001B[39m tfds\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwiki40b/fr\u001B[39m\u001B[38;5;124m\"\u001B[39m,split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m,data_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://tfhub.dev/google/collections/wiki40b-lm/1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\Internship\\lib\\site-packages\\tensorflow_datasets\\__init__.py:43\u001B[0m\n\u001B[0;32m     41\u001B[0m _TIMESTAMP_IMPORT_STARTS \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mabsl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m logging\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_tfds_logging\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlogging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m call_metadata \u001B[38;5;28;01mas\u001B[39;00m _call_metadata\n\u001B[0;32m     46\u001B[0m _metadata \u001B[38;5;241m=\u001B[39m _call_metadata\u001B[38;5;241m.\u001B[39mCallMetadata()\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\Internship\\lib\\site-packages\\tensorflow_datasets\\core\\__init__.py:22\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Allow to use `tfds.core.Path` in dataset implementation which seems more\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# natural than having to import a third party module.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m community\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_builder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BeamBasedBuilder\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_builder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BuilderConfig\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\Internship\\lib\\site-packages\\tensorflow_datasets\\core\\community\\__init__.py:18\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# coding=utf-8\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Copyright 2023 The TensorFlow Datasets Authors.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m\"\"\"Community dataset API.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommunity\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhuggingface_wrapper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mock_builtin_to_use_gfile\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommunity\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhuggingface_wrapper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mock_huggingface_import\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommunity\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mload\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m builder_cls_from_module\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\Internship\\lib\\site-packages\\tensorflow_datasets\\core\\community\\huggingface_wrapper.py:31\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01munittest\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mock\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m epath\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataset_builder\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataset_info\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m download\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\Internship\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:44\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m reader \u001B[38;5;28;01mas\u001B[39;00m reader_lib\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m registered\n\u001B[1;32m---> 44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m split_builder \u001B[38;5;28;01mas\u001B[39;00m split_builder_lib\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m splits \u001B[38;5;28;01mas\u001B[39;00m splits_lib\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf_compat\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\Internship\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:37\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m splits \u001B[38;5;28;01mas\u001B[39;00m splits_lib\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m writer \u001B[38;5;28;01mas\u001B[39;00m writer_lib\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m shard_utils\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typing\u001B[38;5;241m.\u001B[39mTYPE_CHECKING:\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\Internship\\lib\\site-packages\\tensorflow_datasets\\core\\writer.py:33\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lazy_imports_lib\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m naming\n\u001B[1;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m shuffle\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m file_utils\n",
      "File \u001B[1;32mD:\\PYTHON\\ENVS\\Internship\\lib\\site-packages\\tensorflow_datasets\\core\\shuffle.py:20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mresource\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mstruct\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Iterator, List, Optional\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'resource'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'aflw2k3d',\n",
       " 'amazon_us_reviews',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bigearthnet',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_corrupted',\n",
       " 'clevr',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco2014',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'dummy_dataset_shared_generator',\n",
       " 'dummy_mnist',\n",
       " 'emnist',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'gap',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imdb_reviews',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lfw',\n",
       " 'lm1b',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'moving_mnist',\n",
       " 'multi_nli',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_v4',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'pet_finder',\n",
       " 'quickdraw_bitmap',\n",
       " 'resisc45',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'scene_parse150',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tf_flowers',\n",
       " 'titanic',\n",
       " 'trivia_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc2007',\n",
       " 'wikipedia',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'xnli']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
